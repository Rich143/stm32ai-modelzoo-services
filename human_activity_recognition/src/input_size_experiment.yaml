general:
  project_name: human_activity_recognition
  #model_path:  ../../../stm32ai-modelzoo/human_activity_recognition/gmp/ST_pretrainedmodel_public_dataset/WISDM/gmp_wl_48/gmp_wl_48.h5
  logs_dir: logs
  saved_models_dir: saved_models
  display_figures: True
  global_seed: 123
  gpu_memory_limit: 24
  deterministic_ops: True

operation_mode: experiment
#operation_mode: training
#choices=['benchmarking', 'deployment', 'evaluation', 'training', 'chain_tb']

dataset:
  name: pamap2  # wisdm or mobility_v1 or pamap2
  class_names: [stationary, walking, running, cycling] # PAMAP2
  # class_names: [Jogging,Stationary,Stairs,Walking] #[Stationary,Walking,Jogging,Biking] for mobility_v1 #[Jogging,Stationary,Stairs,Walking] for WISDM 
  # class_names: [lying, sitting, standing, walking, running, cycling, nordic walking, watching tv, computer work, car driving, ascending stairs, descending stairs, vacuum cleaning, ironing, folding laundry, house cleaning, playing soccer, rope jumping, other]

  training_path: ../datasets/PAMAP2_Dataset  # Mandatory for all but deployment, and benchmarking
  validation_path: # ../datasets/mobility_v1/train.pkl      # Optional
  validation_split: 0.2   # Optional, default value is 0.2
  test_path: 
  test_split: 0.25      # Optional, default value is 0.25
  seed: 123               # Optional, there is a default seed

preprocessing:
  gravity_rot_sup: True
  normalization: False

training:
  model:
    name: ign # available choices are [ign, gmp]
    input_shape: (24, 3, 1)
  dropout: 0.2
  batch_size: 128
  epochs: 120
  steps_per_epoch: 1000
  optimizer:
    Adam:
      #learning_rate: 0.01
  callbacks:
    LRCosineDecay:
       initial_lr: 0.001   # Initial learning rate.
       hold_steps: 0      # The number of epochs to hold the learning rate constant between two drops.
       decay_steps: 50    # the number of steps to decay over following a cosine function
       end_lr: 1e-5        # The learning rate value reached at the end of the cosine decay phase. 
       verbose: 1          # Verbosity (0 or 1). If set to 1, the learning rate value is printed at the beginning of each epoch.
    EarlyStopping:
      monitor: val_accuracy
      restore_best_weights: true
      patience: 20

##  trained_model_path: trained.h5   # Optional, use it if you want to save the best model at the end of the training to a path of your choice

tools:
   stedgeai:
      version: 10.0.0
      optimization: balanced
      on_cloud: True
      #path_to_stedgeai: C:/Users/<XXXXX>/STM32Cube/Repository/Packs/STMicroelectronics/X-CUBE-AI/<*.*.*>/Utilities/windows/stedgeai.exe
      #path_to_stedgeai: /Applications/STMicroelectronics/STM32CubeMX.app/Contents/MacOs/STM32CubeMX/
      path_to_stedgeai: ~/STM32Cube/Repository/Packs/STMicroelectronics/X-CUBE-AI/8.1.0
   path_to_cubeIDE: /Applications/STM32CubeIDE.app/Contents/MacOS/STM32CubeIDE

benchmarking:
  board: B-U585I-IOT02A

deployment:
  c_project_path: ../../stm32ai_application_code/sensing_thread_x/
  IDE: GCC
  verbosity: 1
  hardware_setup:
    serie: STM32U5
    board: B-U585I-IOT02A

mlflow:
  uri: ./experiments_outputs/mlruns

hydra:
  run:
    dir: ./experiments_outputs/${now:%Y_%m_%d_%H_%M_%S}
